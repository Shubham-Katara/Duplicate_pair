# -*- coding: utf-8 -*-
"""Duplicate Question Pairs-Only BoW, NLP project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tbOEyCARcuLOIpukyNhe2EZBAtU42POH

1. We will have two questions in a dataset, and will have to identify if these questions are duplicate or not.
2. This was a Quora dataset to improve the experience of users.
3. We will use ML, although we can also use Deep Learning.

###Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from bs4 import BeautifulSoup
import re
import warnings
warnings.filterwarnings('ignore')

"""###Uploading and understanding data"""

df = pd.read_csv('train.csv')

df

df.shape

df.sample(10)

# is_duplicate column shows it is an binary classification problem.

"""###Pre-processing

####Basic Pre-processing
"""

df.isnull().sum()

new_df = df.sample(30000,random_state=2)

new_df.isnull().sum()

new_df.head()

new_df.info()

"""Getting completely duplicate questions-"""

print(new_df.duplicated(subset=['id']).sum())
print(new_df.duplicated(subset=['qid1']).sum())
print(new_df.duplicated(subset=['qid2']).sum())
print(new_df.duplicated(subset=['question1']).sum())
print(new_df.duplicated(subset=['question2']).sum())

"""####Balancing of dataset"""

print(new_df['is_duplicate'].value_counts())
new_df['is_duplicate'].value_counts().plot(kind='bar')

"""####Repeated Questions"""

qid = pd.Series(list(new_df['qid1']) + list(new_df['qid2']))
print('Number of Unique Questions:',np.unique(qid).shape[0])
x= qid.value_counts()>1        #this expression giving boolean values
print('Number of Questions getting repeated:',x[x].shape[0])   # x[x] will give only true values.

#print(qid.value_counts().max())
#print(qid.value_counts().min())

plt.hist(qid.value_counts().values,bins=160)
plt.yscale('log')
plt.show()

questions = list(new_df['question1']) + list(new_df['question2'])
#len(questions)

"""####Advance Pre-processing"""

def advance_preprocessing(q):
  q = str(q).lower().strip()

  # Replace special characters with string equivalent.
  q = q.replace('@',' at ')
  q = q.replace('%',' percent ')
  q = q.replace('$',' dollar ')
  q = q.replace('₹',' rupee ')
  q = q.replace('€',' euro ')
  
  # remove pattern '[math]', which appears 900 times
  q = q.replace('[math]','')

  # Replacing some numbers with string equivalents (not perfect, can be done better to account for more cases)
  q = q.replace(',000,000,000 ', 'b ')
  q = q.replace(',000,000 ', 'm ')
  q = q.replace(',000 ', 'k ')
  q = re.sub(r'([0-9]+)000000000', r'\1b', q)
  q = re.sub(r'([0-9]+)000000', r'\1m', q)
  q = re.sub(r'([0-9]+)000', r'\1k', q)

  # Decontracting Words
  contractions = {
      "ain't": "am not",
    "aren't": "are not",
    "can't": "can not",
    "can't've": "can not have",
    "'cause": "because",
    "could've": "could have",
    "couldn't": "could not",
    "couldn't've": "could not have",
    "didn't": "did not",
    "doesn't": "does not",
    "don't": "do not",
    "hadn't": "had not",
    "hadn't've": "had not have",
    "hasn't": "has not",
    "haven't": "have not",
    "he'd": "he would",
    "he'd've": "he would have",
    "he'll": "he will",
    "he'll've": "he will have",
    "he's": "he is",
    "how'd": "how did",
    "how'd'y": "how do you",
    "how'll": "how will",
    "how's": "how is",
    "i'd": "i would",
    "i'd've": "i would have",
    "i'll": "i will",
    "i'll've": "i will have",
    "i'm": "i am",
    "i've": "i have",
    "isn't": "is not",
    "it'd": "it would",
    "it'd've": "it would have",
    "it'll": "it will",
    "it'll've": "it will have",
    "it's": "it is",
    "let's": "let us",
    "ma'am": "madam",
    "mayn't": "may not",
    "might've": "might have",
    "mightn't": "might not",
    "mightn't've": "might not have",
    "must've": "must have",
    "mustn't": "must not",
    "mustn't've": "must not have",
    "needn't": "need not",
    "needn't've": "need not have",
    "o'clock": "of the clock",
    "oughtn't": "ought not",
    "oughtn't've": "ought not have",
    "shan't": "shall not",
    "sha'n't": "shall not",
    "shan't've": "shall not have",
    "she'd": "she would",
    "she'd've": "she would have",
    "she'll": "she will",
    "she'll've": "she will have",
    "she's": "she is",
    "should've": "should have",
    "shouldn't": "should not",
    "shouldn't've": "should not have",
    "so've": "so have",
    "so's": "so as",
    "that'd": "that would",
    "that'd've": "that would have",
    "that's": "that is",
    "there'd": "there would",
    "there'd've": "there would have",
    "there's": "there is",
    "they'd": "they would",
    "they'd've": "they would have",
    "they'll": "they will",
    "they'll've": "they will have",
    "they're": "they are",
    "they've": "they have",
    "to've": "to have",
    "wasn't": "was not",
    "we'd": "we would",
    "we'd've": "we would have",
    "we'll": "we will",
    "we'll've": "we will have",
    "we're": "we are",
    "we've": "we have",
    "weren't": "were not",
    "what'll": "what will",
    "what'll've": "what will have",
    "what're": "what are",
    "what's": "what is",
    "what've": "what have",
    "when's": "when is",
    "when've": "when have",
    "where'd": "where did",
    "where's": "where is",
    "where've": "where have",
    "who'll": "who will",
    "who'll've": "who will have",
    "who's": "who is",
    "who've": "who have",
    "why's": "why is",
    "why've": "why have",
    "will've": "will have",
    "won't": "will not",
    "won't've": "will not have",
    "would've": "would have",
    "wouldn't": "would not",
    "wouldn't've": "would not have",
    "y'all": "you all",
    "y'all'd": "you all would",
    "y'all'd've": "you all would have",
    "y'all're": "you all are",
    "y'all've": "you all have",
    "you'd": "you would",
    "you'd've": "you would have",
    "you'll": "you will",
    "you'll've": "you will have",
    "you're": "you are",
    "you've": "you have"
    }

  q_modified = []
  for word in q.split():
    if word in contractions:
      word = contractions[word]

    q_modified.append(word)
  
  q = ' '.join(q_modified)
  q = q.replace("'ve", " have")
  q = q.replace("n't", " not")
  q = q.replace("'re", " are")
  q = q.replace("'ll", " will")

  # Remove HTML tags
  q = BeautifulSoup(q)
  #get_text() method returns the text inside the Beautiful Soup or Tag object as a single Unicode string.
  q = q.get_text()

  # Remove Punctuations
  pattern = re.compile('\W')
  q = re.sub(pattern, ' ', q).strip()

  return q

advance_preprocessing("I've already! wasn't <b>done</b>?")

new_df['question1'] = new_df['question1'].apply(advance_preprocessing)
new_df['question2'] = new_df['question2'].apply(advance_preprocessing)

new_df.head()

"""###Feature Engineering

####Basic Features
"""

new_df['q1_len'] = new_df['question1'].str.len()
new_df['q2_len'] = new_df['question2'].str.len()

new_df.head()

#new_df['q1_num_words'] = new_df['question1'].apply(lambda row: len(row.split(" ")))
new_df['q1_num_words'] = new_df['question1'].apply(lambda row: len(row.split(" ")))
new_df['q2_num_words'] = new_df['question2'].apply(lambda row: len(row.split(" ")))

new_df['q1_num_words'].head()

# Common Words
def common_words(row):
  w1= set(map(lambda word: word.lower().strip(),row['question1'].split()))
  w2= set(map(lambda word: word.lower().strip(),row['question2'].split()))
  return len(w1 & w2)

# used map b/c lower() and strip functions apply on words.

# Pandas.apply allow the users to pass a function and apply it on every single value of the Pandas series.
new_df['common_words']= new_df.apply(common_words,axis=1)

def total_words(row):
  w1= set(map(lambda word: word.lower().strip(),row['question1'].split()))
  w2= set(map(lambda word: word.lower().strip(),row['question2'].split()))
  return len(w1) + len(w2)

new_df['total_words']= new_df.apply(total_words,axis=1)

new_df['word_share']= round(new_df['common_words']/new_df['total_words'],2)

"""####Advance Features"""

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

def fetch_token_features(row):

  q1 = row['question1']
  q2 = row['question2']

  Stopwords = stopwords.words("english")

  SAFE_DIV = 0.0001

  token_features = [0.0]*8

  # Tokens
  q1_tokens = q1.split()
  q2_tokens = q2.split()

  if len(q1_tokens)==0 or len(q2_tokens)==0:
    return token_features

  # Words
  q1_words = set([word for word in q1_tokens if word not in Stopwords])
  q2_words = set([word for word in q2_tokens if word not in Stopwords])

  # Stopwords
  q1_stopwords = set([word for word in q1_tokens if word in Stopwords])
  q2_stopwords = set([word for word in q2_tokens if word in Stopwords])

  # features
  token_features[0] = len(q1_words & q2_words)/(min(len(q1_words),len(q2_words)) + SAFE_DIV)
  token_features[1] = len(q1_words & q2_words)/(max(len(q1_words),len(q2_words)) + SAFE_DIV)
  token_features[2] = len(q1_stopwords & q2_stopwords)/(min(len(q1_stopwords),len(q2_stopwords)) + SAFE_DIV)
  token_features[3] = len(q1_stopwords & q2_stopwords)/(max(len(q1_stopwords),len(q2_stopwords)) + SAFE_DIV)
  token_features[4] = len(set(q1_tokens) & set(q2_tokens))/(min(len(q1_tokens),len(q2_tokens)) + SAFE_DIV)
  token_features[5] = len(set(q1_tokens) & set(q2_tokens))/(max(len(q1_tokens),len(q2_tokens)) + SAFE_DIV)
  token_features[6] = int(q1_tokens[-1]==q2_tokens[-1])
  token_features[7] = int(q1_tokens[0]==q2_tokens[0])

  return token_features

token_features = new_df.apply(fetch_token_features,axis=1)  # type: pandas.core.series.Series

type(token_features)

new_df['cwc_min'] = list(map(lambda x:x[0],token_features))
new_df['cwc_max'] = list(map(lambda x:x[1],token_features))
new_df['csc_min'] = list(map(lambda x:x[2],token_features))
new_df['csc_max'] = list(map(lambda x:x[3],token_features))
new_df['ctc_min'] = list(map(lambda x:x[4],token_features))
new_df['ctc_max'] = list(map(lambda x:x[5],token_features))
new_df['last_word_eq'] = list(map(lambda x:x[6],token_features))
new_df['first_word_eq'] = list(map(lambda x:x[7],token_features))

!pip install distance
import distance

def length_based_feature(row):
  length_features = [0.0]*3

  q1 = row['question1']
  q2 = row['question2']
  
  q1_tokens = row['question1'].split()
  q2_tokens = row['question2'].split()
  
  if len(q1_tokens)==0 or len(q2_tokens)==0:
    return length_features

  # Features
  length_features[0] = (len(q1_tokens)+len(q2_tokens))/2
  length_features[1] = abs(len(q1_tokens)-len(q2_tokens))
  strs = list(distance.lcsubstrings(q1, q2))     # lcsubstrings: longest common substring
  length_features[2] = len(strs[0]) / (min(len(q1), len(q2)) + 1)

  return length_features

length_features = new_df.apply(length_based_feature,axis=1)

new_df['mean_length'] = list(map(lambda x:x[0],length_features))
new_df['abs_length'] = list(map(lambda x:x[1],length_features))
new_df['longest_substr_ratio'] = list(map(lambda x:x[2],length_features))

!pip install fuzzywuzzy
from fuzzywuzzy import fuzz

def fetch_fuzzy_features(row):

  q1 = row['question1']
  q2 = row['question2']

  fuzzy_features = [0.0]*4
  fuzzy_features[0] = fuzz.QRatio(q1,q2)
  fuzzy_features[1] = fuzz.partial_ratio(q1,q2)
  fuzzy_features[2] = fuzz.token_sort_ratio(q1,q2)
  fuzzy_features[3] = fuzz.token_set_ratio(q1,q2)

  return fuzzy_features

fuzzy_features = new_df.apply(fetch_fuzzy_features,axis=1)

new_df['fuzz_ratio'] = list(map(lambda x:x[0],fuzzy_features))
new_df['fuzz_partial_ratio'] = list(map(lambda x:x[1],fuzzy_features))
new_df['token_sort_ratio'] = list(map(lambda x:x[2],fuzzy_features))
new_df['token_set_ratio'] = list(map(lambda x:x[3],fuzzy_features))

pd.options.display.max_columns = None
#pd.options.display.max_rows = None

print(new_df.shape)
new_df.head()

"""###Feature Analysis

####Basic Feature Analysis
"""

sns.distplot(new_df['q1_len'])
print('minimum characters:',new_df['q1_len'].min())
print('maximum characters:',new_df['q1_len'].max())
print('avg no. of characters:',new_df['q1_len'].mean())

sns.distplot(new_df['q2_len'])
print('minimum characters:',new_df['q2_len'].min())
print('maximum characters:',new_df['q2_len'].max())
print('avg no. of characters:',new_df['q2_len'].mean())

sns.displot(new_df['q1_num_words'])
print('minimum words',new_df['q1_num_words'].min())
print('maximum words',new_df['q1_num_words'].max())
print('average num of words',int(new_df['q1_num_words'].mean()))

sns.displot(new_df['q2_num_words'])
print('minimum words',new_df['q2_num_words'].min())
print('maximum words',new_df['q2_num_words'].max())
print('average num of words',int(new_df['q2_num_words'].mean()))

sns.distplot(new_df[new_df['is_duplicate']==0]['common_words'],label='non duplicate')
sns.distplot(new_df[new_df['is_duplicate']==1]['common_words'],label='duplicate')
plt.legend()
plt.show()

sns.distplot(new_df[new_df['is_duplicate']==0]['total_words'],label='non duplicate')
sns.distplot(new_df[new_df['is_duplicate']==1]['total_words'],label='duplicate')
plt.legend()
plt.show()

sns.distplot(new_df[new_df['is_duplicate']==0]['word_share'],label='non duplicate')
sns.distplot(new_df[new_df['is_duplicate']==1]['word_share'],label='duplicate')
plt.legend()
plt.show()

"""####Advance Feature Analysis"""

sns.pairplot(new_df[['cwc_min','csc_min','ctc_min','is_duplicate']],hue='is_duplicate')

sns.pairplot(new_df[['ctc_max', 'cwc_max', 'csc_max', 'is_duplicate']],hue='is_duplicate')

sns.pairplot(new_df[['last_word_eq', 'first_word_eq', 'is_duplicate']],hue='is_duplicate')

sns.pairplot(new_df[['mean_length', 'abs_length','longest_substr_ratio', 'is_duplicate']],hue='is_duplicate')

sns.pairplot(new_df[['fuzz_ratio', 'fuzz_partial_ratio','token_sort_ratio','token_set_ratio', 'is_duplicate']],hue='is_duplicate')

"""####Using t-SNE"""

from sklearn.preprocessing import MinMaxScaler

X = MinMaxScaler().fit_transform(new_df[['cwc_min', 'cwc_max', 'csc_min', 'csc_max' , 'ctc_min' , 'ctc_max' , 'last_word_eq', 'first_word_eq' , 'abs_length' , 'mean_length' , 'token_set_ratio' , 'token_sort_ratio' ,  'fuzz_ratio' , 'fuzz_partial_ratio' , 'longest_substr_ratio']])
y = new_df['is_duplicate'].values

from sklearn.manifold import TSNE
# transforming into 2-D
tsne2d = TSNE(
    n_components=2,
    init='random', # pca
    random_state=101,
    method='barnes_hut',
    n_iter=1000,
    verbose=2,
    angle=0.5
).fit_transform(X)

print(type(tsne2d))
tsne2d.shape

x_df = pd.DataFrame({'x':tsne2d[:,0], 'y':tsne2d[:,1] ,'label':y})
print(x_df.shape)
x_df.head()

sns.lmplot(data=x_df, x='x', y='y', hue='label', fit_reg=False, size=8,palette="Set1",markers=['s','o'])

"""###Count Vectorization"""

questions = list(new_df['question1']) + list(new_df['question2'])
len(questions)

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=3000)
arr = cv.fit_transform(questions).toarray()
q1_arr, q2_arr = np.vsplit(arr,2)

q2_arr.shape

temp_df1 = pd.DataFrame(q1_arr,index=new_df.index)
temp_df2 = pd.DataFrame(q2_arr,index=new_df.index)
temp_df = pd.concat([temp_df1,temp_df2],axis=1)
temp_df.shape

temp_df[temp_df.index==398782]

new_df[new_df.index==398782]

temp_df['is_duplicate']= new_df['is_duplicate']

temp_df

"""###New Dataframe"""

final_df1= new_df.drop(columns= ['is_duplicate','id','qid1','qid2','question1','question2'])
print(final_df1.shape)
final_df1.head()

final_df= pd.concat([final_df1,temp_df],axis=1)
print(final_df.shape)
final_df.head()

"""
###Train Test Split"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(final_df.iloc[:,:-1].values,final_df.iloc[:,-1].values,test_size=0.2,random_state=2)

# Will split both the arrays into train and test

"""###Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
rf = RandomForestClassifier()
rf.fit(x_train,y_train)
y_pred = rf.predict(x_test)
accuracy_score(y_test,y_pred)

"""###xgBoost"""

from xgboost import XGBClassifier
xg = XGBClassifier()
xg.fit(x_train,y_train)
y_pred1 = xg.predict(x_test)
accuracy_score(y_test,y_pred1)

"""### Confusion Matrix"""

from sklearn.metrics import confusion_matrix

confusion_matrix(y_test,y_pred)  # for Random Forest

confusion_matrix(y_test,y_pred1)  # for xgBoost

